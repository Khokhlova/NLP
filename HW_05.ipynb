{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прошу прощения, но не смогла выполнить задание в срок. \n",
    "Споткнулась буквально сразу же - не могу найти размеченные данные на русском языке для Unigram/Bigram Tagger аналогичные brown (как прикрутить сюда данные из corus я не очень поняла), если брать неразмеченные, не знаю как правильно оценить качество.\n",
    "\n",
    "Я очень стараюсь победить эту проблему и выполнить задание, буду очень благодарна если датите мне еще несколько дней на это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-tagger и NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1. Написать теггер на данных с русским языком\n",
    "- проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
    "- написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "- сравнить все реализованные методы сделать выводы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c anaconda wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# Импорт библиотек\n",
    "\n",
    "import corus\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tagged = tagging(data)\n",
    "data_tagged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tagging(sent):\n",
    "#     sent = nltk.word_tokenize(sent)\n",
    "#     sent = nltk.pos_tag(sent)\n",
    "#     return sent\n",
    "\n",
    "# data_tagged = tagging(data)\n",
    "# data_tagged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбивка данных на тренировочную и тестовые выборки\n",
    "\n",
    "# train_data = []\n",
    "# test_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnigramTagger\n",
    "\n",
    "# unigram_tagger = UnigramTagger(train_data)\n",
    "# display(unigram_tagger.tag(test_sent), unigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigramTagger\n",
    "\n",
    "# bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
    "# display(bigram_tagger.tag(test_sent), bigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrigramTagger\n",
    "\n",
    "# trigram_tagger = TrigramTagger(train_data, backoff=bigram_tagger)\n",
    "# display(trigram_tagger.tag(test_sent), trigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Комбинация тэггеров\n",
    "\n",
    "# def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "#     for cls in tagger_classes:\n",
    "#         backoff = cls(train_sents, backoff=backoff)\n",
    "#     return backoff\n",
    "\n",
    "\n",
    "# backoff = DefaultTagger('NN') \n",
    "# tag = backoff_tagger(train_data,  \n",
    "#                      [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "#                      backoff = backoff) \n",
    "  \n",
    "# tag.evaluate(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2. Проверить насколько хорошо работает NER\n",
    "данные брать из http://www.labinform.ru/pub/named_entities/\n",
    "- проверить NER из nltk/spacy/deeppavlov\n",
    "- написать свой нер попробовать разные подходы\n",
    "-- передаём в сетку токен и его соседей\n",
    "-- передаём в сетку только токен\n",
    "- сделать выводы по вашим экспериментам какой из подходов успешнее справляется\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
